{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic(x, name, in_channel, out_channel):\n",
    "    conv0 = layer.bn_relu_conv(x, name + \"_basic0\", in_channel, out_channel, 3)\n",
    "    conv1 = layer.bn_relu_conv(conv0, name + \"_basic1\", out_channel, out_channel, 3) \n",
    "    return conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bottle_neck(x, name, in_channel, out_channel):\n",
    "    conv0 = layer.bn_relu_conv(x, name + \"_bottleneck0\", in_channel, out_channel, 3)\n",
    "    conv1 = layer.bn_relu_conv(conv0, name + \"_bottleneck1\", out_channel, out_channel / 2, 3)\n",
    "    conv2 = layer.bn_relu_conv(conv1, name + \"_bottleneck2\", out_channel / 2, out_channel, 3)\n",
    "    return conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_wide(x, name, in_channel, out_channel):\n",
    "    conv0 = layer.bn_relu_conv(x, name + \"_basicwide0\", in_channel, out_channel, 3)\n",
    "    conv1 = layer.bn_relu_conv(conv0, name + \"_basicwide1\", out_channel, out_channel, 3) \n",
    "    return conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(x, name, prob, in_channel, out_channel):\n",
    "    conv0 = layer.bn_relu_conv(x, name + \"_dropout_conv0\", in_channel, out_channel, 3)\n",
    "    dropout1 = tf.nn.dropout(conv0, prob, name = name + \"_dropout\")\n",
    "    conv2 = layer.bn_relu_conv(dropout1, name + \"_dropout_conv1\", out_channel, out_channel, 3)\n",
    "    return conv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build WideResnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_wide_resnet(x, num_classes, N, k, block, prob = None):\n",
    "    channels = [3, 16, 16 * k, 32 * k, 64 * k]\n",
    "    layers = []\n",
    "\n",
    "    # conv1\n",
    "    # conv1 = layer.bn_relu_conv(x, \"conv1\", channels[0], channels[1], 3)\n",
    "    conv1 = layer.conv_bn_relu(x, \"conv1\", channels[0], channels[1], 3)\n",
    "    layers.append(conv1)\n",
    "\n",
    "    # conv2\n",
    "    # 1st\n",
    "    before20 = layers[-1]\n",
    "    conv20 = layer.conv_layer(before20, \"conv20\", [1, 1, channels[1], channels[2]])\n",
    "    # conv20b = block(before20, \"conv20b\", prob, channels[1], channels[2]) if block is dropout else block(before20, \"conv20b\", channels[1], channels[2])\n",
    "    conv20b_ = layer.conv_bn_relu(before20, \"conv20b_\", channels[1], channels[2], 3)\n",
    "    conv20b = layer.conv_layer(conv20b_, \"conv20b\", [3, 3, channels[2], channels[2]])\n",
    "    output20 = layer.bn_relu(conv20 + conv20b, \"output20\")\n",
    "    layers.append(output20)\n",
    "\n",
    "    # others\n",
    "    for n in range(1, N):\n",
    "        before2n = tf.identity(layers[-1])\n",
    "        # conv2n = layer.conv_layer(before2n, \"conv2%d\" % n, [3, 3, channels[2], channels[2]])\n",
    "        conv2nb = block(layers[-1], \"conv2%db\" % n, prob, channels[2], channels[2]) if block is dropout else block(layers[-1], \"conv2%db\" % n, channels[2], channels[2])\n",
    "        output2n = layer.bn_relu(before2n + conv2nb, \"output2%d\" % n)\n",
    "        layers.append(output2n)\n",
    "\n",
    "    # downsampling0\n",
    "    #downsampling0 = layer.avg_pool_layer(layers[-1], \"downsampling0\", [1, 2, 2, 1])\n",
    "    downsampling0 = layer.max_pool_layer(layers[-1], \"downsampling0\", [1, 2, 2, 1])\n",
    "    layers.append(downsampling0)\n",
    "\n",
    "    # conv3\n",
    "    # 1st\n",
    "    before30 = layers[-1]\n",
    "    conv30 = layer.conv_layer(before30, \"conv30\", [1, 1, channels[2], channels[3]])\n",
    "    # conv30b = block(before30, \"conv30b\", prob, channels[2], channels[3]) if block is dropout else block(before30, \"conv30b\", channels[2], channels[3])\n",
    "    conv30b_ = layer.conv_bn_relu(before30, \"conv30b_\", channels[2], channels[3], 3)\n",
    "    conv30b = layer.conv_layer(conv30b_, \"conv30b\", [3, 3, channels[3], channels[3]])\n",
    "    output30 = layer.bn_relu(conv30 + conv30b, \"output30\")\n",
    "    layers.append(output30)\n",
    "\n",
    "    # others\n",
    "    for n in range(1, N):\n",
    "        before3n = tf.identity(layers[-1])\n",
    "        # conv3n = layer.conv_layer(before3n, \"conv3%d\" % n, [3, 3, channels[3], channels[3]])\n",
    "        conv3nb = block(layers[-1], \"conv3%db\" % n, prob, channels[3], channels[3]) if block is dropout else block(layers[-1], \"conv3%db\" % n, channels[3], channels[3])\n",
    "        output3n = layer.bn_relu(before3n + conv3nb, \"output3%d\" % n)\n",
    "        layers.append(output3n)\n",
    "\n",
    "    # downsampling1\n",
    "    #downsampling1 = layer.avg_pool_layer(layers[-1], \"downsampling1\", [1, 2, 2, 1])\n",
    "    downsampling1 = layer.max_pool_layer(layers[-1], \"downsampling1\", [1, 2, 2, 1])\n",
    "    layers.append(downsampling1)\n",
    "\n",
    "    # conv4\n",
    "    # 1st\n",
    "    before40 = layers[-1]\n",
    "    conv40 = layer.conv_layer(before40, \"conv40\", [1, 1, channels[3],channels[4]])\n",
    "    # conv40b = block(before40, \"conv40b\", prob, channels[3], channels[4]) if block is dropout else block(before40, \"conv40b\", channels[3], channels[4])\n",
    "    conv40b_ = layer.conv_bn_relu(before40, \"conv40b_\", channels[3], channels[4], 3)\n",
    "    conv40b = layer.conv_layer(conv40b_, \"conv40b\", [3, 3, channels[4], channels[4]])\n",
    "    output40 = layer.bn_relu(conv40 + conv40b, \"output40\")\n",
    "    layers.append(output40)\n",
    "\n",
    "    # others\n",
    "    for n in range(1, N):\n",
    "        before4n = tf.identity(layers[-1])\n",
    "        # conv4n = layer.conv_layer(before4n, \"conv4%d\" % n, [3, 3, channels[4], channels[4]])\n",
    "        conv4nb = block(layers[-1], \"conv4%db\" % n, prob, channels[4], channels[4]) if block is dropout else block(layers[-1], \"conv4%db\" % n, channels[4], channels[4])\n",
    "        output4n = layer.bn_relu(before4n + conv4nb, \"output4%d\" % n)\n",
    "        layers.append(output4n)\n",
    "\n",
    "    # avg pooling\n",
    "    avg_pool = layer.avg_pool_layer(layers[-1], name = \"avg_pool\", pooling_size = [1, 8, 8, 1])\n",
    "    layers.append(avg_pool)\n",
    "\n",
    "    # flatten and fully connected\n",
    "    flatten = layer.flatten_layer(layers[-1])\n",
    "    fc = layer.fc_layer(flatten, num_classes, \"fc\")\n",
    "    layers.append(fc)\n",
    "    \n",
    "    sm = tf.nn.softmax(layers[-1], name = \"prediction\")\n",
    "    layers.append(sm)\n",
    "\n",
    "    return layers[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'init_lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-e3c550ce73b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_lr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_decay_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_decay_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_decay_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_decay_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"global_step\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m self.learning_rate = tf.train.exponential_decay(self.init_lr, self.global_step, \n",
      "\u001b[0;31mNameError\u001b[0m: name 'init_lr' is not defined"
     ]
    }
   ],
   "source": [
    "self.init_lr = init_lr\n",
    "self.lr_decay_step = lr_decay_step\n",
    "self.lr_decay_rate = lr_decay_rate\n",
    "self.global_step = tf.Variable(0, trainable = False, name = \"global_step\")\n",
    "self.learning_rate = tf.train.exponential_decay(self.init_lr, self.global_step, \n",
    "                                                self.lr_decay_step, self.lr_decay_rate, staircase = True, name = \"learning_rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(self, pred, gt):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels = gt, logits = pred), name = \"loss\")\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    return loss\n",
    "\n",
    "pred = wrn.build_wide_resnet(self.input, self.num_classes, self.N, self.k, wrn.dropout, prob = self.dropout)\n",
    "loss = self.loss_func(pred, self.gt)\n",
    "evaluation = self.evaluate(logits = pred, labels = self.gt, rank = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(self, logits, labels, rank):\n",
    "    correct = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name = \"accuracy\")\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    return accuracy\n",
    "\n",
    "pred = wrn.build_wide_resnet(self.input, self.num_classes, self.N, self.k, wrn.dropout, prob = self.dropout)\n",
    "evaluation = self.evaluate(logits = pred, labels = self.gt, rank = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_with_weight_decay = tf.contrib.opt.extend_with_decoupled_weight_decay(tf.train.MomentumOptimizer)\n",
    "optimizer = optimizer_with_weight_decay(weight_decay = self.weight_decay,\n",
    "                                        learning_rate = self.learning_rate,\n",
    "                                        momentum = 0.9).minimize(loss, global_step = self.global_step, name = \"momentum_minimizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle_cifar10(path):\n",
    "    import cPickle\n",
    "    with open(path, 'rb') as f:\n",
    "        dic = cPickle.load(f)\n",
    "        return dic\n",
    "\n",
    "def create_dataset(data, label, bs, repeat_size = None):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((data, label))\n",
    "    dataset = dataset.shuffle(len(label))\n",
    "    dataset = dataset.map(map_batch)\n",
    "    if repeat_size is None:\n",
    "        dataset = dataset.batch(bs).repeat()\n",
    "    else:\n",
    "        dataset = dataset.batch(bs).repeat(repeat_size)\n",
    "    return dataset\n",
    "\n",
    "def map_batch(data_batch, label_batch):\n",
    "    data = tf.transpose(tf.reshape(data_batch, config.ori_size), [1, 2, 0])\n",
    "    label = tf.one_hot(label_batch, config.num_classes)\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "data = []\n",
    "label = []\n",
    "for cdir in self.dataset_dirs:\n",
    "    dic = cifar.unpickle_cifar10(cdir)\n",
    "    data.append(dic['data'])\n",
    "    label.append(dic['labels'])\n",
    "data = np.concatenate(data, axis = 0)\n",
    "label = np.concatenate(label, axis = 0)\n",
    "print data.shape\n",
    "print label.shape\n",
    "dataset = cifar.create_dataset(data, label, self.bs)\n",
    "batch_tensor = cifar.get_next(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print parameter number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "while i < self.epoch:\n",
    "    batch = sess.run(batch_tensor)\n",
    "    if len(batch[1]) != self.bs:\n",
    "        continue\n",
    "        \n",
    "    summary, gs, l, eva, lr, _ = sess.run([merged, self.global_step, loss, evaluation, self.learning_rate, optimizer],\n",
    "                                          feed_dict = {self.input : batch[0],\n",
    "                                                       self.gt : batch[1],\n",
    "                                                       self.dropout : 0.3,\n",
    "                                                       self.weight_decay : 0.0005})\n",
    "    \n",
    "    train_writer.add_summary(summary, gs)\n",
    "    print \"Global steps %d -- loss = %.6f, lr = %.9f, acc = %.6f\" % (gs, l, lr, eva)\n",
    "    \n",
    "    if gs % self.save_model_step == 0:\n",
    "        saver.save(sess, self.model_dir, global_step = self.save_model_step)\n",
    "        print \"Save the model successfully!\"\n",
    "        \n",
    "    i += 1\n",
    "\n",
    "train_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read pre-trained model\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph(graph_path)\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(ckpt_path))\n",
    "    \n",
    "    graph = tf.get_default_graph()\n",
    "    x = graph.get_tensor_by_name(input_name)\n",
    "    gt = graph.get_tensor_by_name(gt_name)\n",
    "    pred = graph.get_tensor_by_name(pred_name)\n",
    "    accuracy = graph.get_tensor_by_name(\"accuracy:0\")\n",
    "    dropout = graph.get_tensor_by_name(\"dropout:0\")\n",
    "    weight_decay = graph.get_tensor_by_name(\"weight_decay:0\")\n",
    "    a = []\n",
    "    \n",
    "    b = 1\n",
    "    while True:\n",
    "        try:\n",
    "            batch = sess.run(batch_tensor)\n",
    "            if len(batch[1]) != bs:\n",
    "                break\n",
    "                \n",
    "            acc = sess.run([accuracy], feed_dict = {x : batch[0], gt : batch[1], dropout : 1.0, weight_decay : 1.0})\n",
    "            a += acc\n",
    "            print \"The average accuracy of the %d batch is %.6f.\" % (b, np.mean(acc))\n",
    "            b += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "            \n",
    "    print \"The average accuracy of all batches is %.6f.\" % np.mean(a)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
